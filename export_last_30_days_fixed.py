#!/usr/bin/env python3
"""
Export last 30 days timesheet with fixed pagination
"""

import logging
from meegle_sdk import MeegleSDK
from meegle_business.timeline.view_extractor import ViewTimelineExtractor
from meegle_business.export.csv_exporter import CSVExporter

# Configure logging
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(name)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

def export_last_30_days_timesheet_fixed():
    """Export last 30 days timesheet with fixed pagination"""
    
    print("æ­£åœ¨å¯¼å‡ºè¿‘30å¤© timesheet (ä¿®å¤åˆ†é¡µå)...")
    print("=" * 50)
    
    try:
        # Initialize SDK, extractor, and exporter
        sdk = MeegleSDK()
        extractor = ViewTimelineExtractor(sdk)
        exporter = CSVExporter()
        
        # Use the correct view ID
        view_id = "OXnKluvHR"
        
        # Show date range that will be processed
        start_date, end_date = extractor._get_last_n_days_range(30)
        print(f"æ—¥æœŸèŒƒå›´: {start_date} åˆ° {end_date}")
        print(f"è§†å›¾ ID: {view_id}")
        
        # First check how many work items we'll process
        print("\n1. æ£€æŸ¥è§†å›¾ä¸­çš„å·¥ä½œé¡¹æ•°é‡:")
        all_work_item_ids = sdk.work_items.get_all_work_items_in_view(view_id)
        print(f"   è§†å›¾ä¸­æ€»å·¥ä½œé¡¹æ•°é‡: {len(all_work_item_ids)}")
        print()
        
        # Extract timeline data for last 30 days
        print("2. å¼€å§‹æå–è¿‘30å¤©çš„æ—¶é—´çº¿æ•°æ®...")
        timeline_data = extractor.extract_timeline_last_n_days(
            view_id=view_id,
            days=30,
            work_item_type_key="story"
        )
        
        if not timeline_data.entries:
            print("âŒ æ²¡æœ‰æ‰¾åˆ°æ—¶é—´çº¿æ•°æ®")
            return
        
        print(f"âœ… æˆåŠŸæå– {len(timeline_data.entries)} æ¡æ—¶é—´çº¿è®°å½•")
        print(f"   æ€»å·¥æ—¶: {timeline_data.total_hours:.2f} å°æ—¶")
        print(f"   å‚ä¸äººæ•°: {timeline_data.unique_users} äºº")
        print(f"   æ—¥æœŸèŒƒå›´: {timeline_data.date_range}")
        print()
        
        # Export to CSV
        print("3. æ­£åœ¨å¯¼å‡ºåˆ° CSV æ–‡ä»¶...")
        csv_path = exporter.export_timeline_to_csv(timeline_data, "last_30_days_timesheet_fixed.csv")
        
        print(f"âœ… æˆåŠŸå¯¼å‡ºåˆ°: {csv_path}")
        print(f"   æ–‡ä»¶å¤§å°: {csv_path.stat().st_size} å­—èŠ‚")
        
        # Show extraction statistics
        stats = extractor.get_extraction_stats()
        print(f"\nğŸ“Š æå–ç»Ÿè®¡:")
        print(f"   å·¥ä½œæµç¼“å­˜: {stats['workflow_cache_size']} é¡¹")
        print(f"   å·¥ä½œé¡¹ç¼“å­˜: {stats['work_item_cache_size']} é¡¹")
        print(f"   å¤±è´¥çš„é¡¹ç›®æŸ¥æ‰¾: {stats['failed_project_ids_count']} ä¸ª")
        
        # Compare with previous export
        print(f"\nğŸ“ˆ å¯¹æ¯”:")
        print(f"   ä¿®å¤å‰: å¤„ç† 200 ä¸ªå·¥ä½œé¡¹")
        print(f"   ä¿®å¤å: å¤„ç† {len(all_work_item_ids)} ä¸ªå·¥ä½œé¡¹")
        print(f"   å¢åŠ : {len(all_work_item_ids) - 200} ä¸ªå·¥ä½œé¡¹ (+{((len(all_work_item_ids) - 200) / 200 * 100):.1f}%)")
        
    except Exception as e:
        print(f"âŒ å¯¼å‡ºå¤±è´¥: {e}")
        logger.error(f"Export failed: {e}", exc_info=True)

if __name__ == "__main__":
    export_last_30_days_timesheet_fixed() 